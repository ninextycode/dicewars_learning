{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib ipympl\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import tqdm\n",
    "import time\n",
    "import csv\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, ConcatDataset, random_split\n",
    "\n",
    "from learning.data.process_utils import move_to_device\n",
    "from learning.data.av2_utils import extract_input, collate_av2_data\n",
    "from learning.data.av2_utils import extract_output_target\n",
    "from learning.data.av2_dataset import ActionValueV2Dataset\n",
    "from learning.model.actionvalue_v2 import ActionValueModelV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955fd39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb39e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed seed for reproducibility\n",
    "seed = 21\n",
    "torch.manual_seed(seed)\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "history_folder = \"server/history/\"\n",
    "history_files = sorted(glob.glob(os.path.join(history_folder, \"history_*.json\")))\n",
    "\n",
    "datasets = [ActionValueV2Dataset(f) for f in history_files]\n",
    "concatenated_dataset = ConcatDataset(datasets)\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(\n",
    "    concatenated_dataset,\n",
    "    [0.8, 0.1, 0.1],\n",
    "    generator=generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43efe07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, 10000, pin_memory=True, shuffle=True, \n",
    "                         collate_fn=collate_av2_data, generator=generator)\n",
    "val_loader = DataLoader(val_ds, 20000, pin_memory=True, shuffle=True, \n",
    "                       collate_fn=collate_av2_data, generator=generator)\n",
    "\n",
    "print(\"train_ds\", len(train_ds))\n",
    "print(\"val_ds\", len(val_ds))\n",
    "print(\"test_ds\", len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9e03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e1a9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a966078",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = None\n",
    "optim_state = None\n",
    "latest_epoch = -1\n",
    "\n",
    "freeze = False\n",
    "\n",
    "model_name = \"actionvalue_v2\" + (\"_no_freeze\" if not freeze else \"\")\n",
    "models_dir = f\"learning/{model_name}_checkpoints\" \n",
    "\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Find the latest checkpoint file (with highest epoch number)\n",
    "checkpoint_files = glob.glob(os.path.join(models_dir, f\"{model_name}_*.pt\"))\n",
    "if checkpoint_files:\n",
    "    # Extract epoch numbers from filenames\n",
    "    epoch_nums = [int(f.split(\"_\")[-1].split(\".\")[0]) for f in checkpoint_files]\n",
    "    latest_epoch = max(epoch_nums)\n",
    "    latest_checkpoint = os.path.join(models_dir, f\"{model_name}_{latest_epoch:06}.pt\")\n",
    "    print(f\"Loading latest checkpoint: {latest_checkpoint} (epoch {latest_epoch})\")\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=torch.device('cpu'))\n",
    "    model_state = checkpoint[\"model_state\"]\n",
    "    optim_state = checkpoint[\"optim_state\"]\n",
    "else:\n",
    "    print(\"No valid checkpoint files found\")\n",
    "    \n",
    "start_epoch = latest_epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5463a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "action_value_model = ActionValueModelV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_from_old_model = (model_state is None)\n",
    "\n",
    "if init_from_old_model:\n",
    "    print(\"⚠️⚠️⚠️ Warning, model is being initialized from an old checkpoint\")\n",
    "    \n",
    "    checkpoint = torch.load(\n",
    "        \"learning/old_models/nodevalue_full.pt\",\n",
    "        map_location=torch.device('cpu')\n",
    "    )\n",
    "    model_state = checkpoint[\"model_state\"]\n",
    "\n",
    "    # update old model state to match the new one\n",
    "    mismatch = action_value_model.load_state_dict(model_state, strict=False)\n",
    "    model_state = action_value_model.state_dict()\n",
    "\n",
    "    missing_keys, unexpected_keys = mismatch\n",
    "\n",
    "    for mk in missing_keys:\n",
    "        print(\"missing key\", mk)\n",
    "\n",
    "    for uk in unexpected_keys:\n",
    "        print(\"unexpected key\", uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5de2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model state if available\n",
    "if model_state is not None:\n",
    "    action_value_model.load_state_dict(model_state)\n",
    "    print(f\"Model state loaded from checkpoint.\")\n",
    "\n",
    "if freeze:\n",
    "    # Only make attack_value_fn and end_turn_value_fn parameters trainable\n",
    "    for name, param in action_value_model.named_parameters():\n",
    "        if name.startswith('attack_value_fn') or name.startswith('end_turn_value_fn'):\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        \n",
    "trainable_params = sum(p.numel() for p in action_value_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in action_value_model.parameters())\n",
    "print(f\"Model created. Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "print(\"Only attack_value_fn and end_turn_value_fn modules are trainable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5403d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details about which specific parameters are trainable\n",
    "print(\"Detailed parameter trainable status:\")\n",
    "for name, param in action_value_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"✓ TRAINABLE: {name}, shape={param.shape}\")\n",
    "    else:\n",
    "        # For frozen parameters, just count by module\n",
    "        module = name.split('.')[0]\n",
    "        print(f\"✗ FROZEN: {name}, shape={param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f25026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── CSV set‑up: log one row per epoch ───────────────────────────────────────\n",
    "csv_path = f\"{model_name}_epoch_metrics.csv\"\n",
    "epoch_log_file_exists = os.path.isfile(csv_path)\n",
    "if epoch_log_file_exists and os.path.getsize(csv_path) == 0:\n",
    "    os.remove(csv_path)\n",
    "    epoch_log_file_exists = False\n",
    "\n",
    "csv_file = open(csv_path, \"a\", newline=\"\")  # Open in append mode\n",
    "csv_writer = csv.writer(csv_file)\n",
    "\n",
    "# Only write header if file doesn't exist yet\n",
    "if not epoch_log_file_exists:\n",
    "    csv_writer.writerow([\n",
    "        \"epoch\",           # 0‑based epoch index\n",
    "        \"train_loss\",      # average training loss for the epoch\n",
    "        \"val_loss\",        # average validation loss for the epoch\n",
    "        \"train_time_sec\",  # seconds spent in training phase\n",
    "        \"val_time_sec\",    # seconds spent in validation phase\n",
    "        \"total_time_sec\"   # train + val\n",
    "    ])\n",
    "\n",
    "# ── model / optimizer prep ─────────────────────────────────────────────────\n",
    "\n",
    "n_epochs   = 10000\n",
    "device     = torch.device(\"cuda\", 0)\n",
    "train_time_limit = 100 * 60\n",
    "val_time_limit = train_time_limit / 5\n",
    "criterion  = nn.MSELoss()\n",
    "\n",
    "action_value_model = action_value_model.to(device)\n",
    "\n",
    "optimizer  = torch.optim.Adam(\n",
    "    [p for p in action_value_model.parameters() if p.requires_grad],\n",
    "    lr=3e-4\n",
    ")\n",
    "reset_optimizer = True\n",
    "if not reset_optimizer and optim_state is not None:\n",
    "    optimizer.load_state_dict(optim_state)\n",
    "\n",
    "scheduler  = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\",\n",
    "    factor=0.25,\n",
    "    patience=10,\n",
    "    threshold=0.0001,\n",
    "    min_lr=1e-8\n",
    ")\n",
    "\n",
    "mean_val = action_value_model.mean_val\n",
    "std_val = action_value_model.std_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea054f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── INITIAL (untrained) LOSS EVALUATION ───────────────────────────────────────\n",
    "if not epoch_log_file_exists:\n",
    "    action_value_model.eval()\n",
    "    with torch.no_grad():\n",
    "        t_start = time.time()\n",
    "\n",
    "        # avg loss on training set *without* gradient tracking\n",
    "        init_train_sum, init_train_batches = 0.0, 0\n",
    "        for b in tqdm.tqdm(train_loader, desc=\"init‑train\"):\n",
    "            b = move_to_device(b, device)\n",
    "            out = action_value_model(*extract_input(b))\n",
    "            o, t = extract_output_target(b, out, mean_val, std_val)\n",
    "            init_train_sum += criterion(o, t).item()\n",
    "            init_train_batches += 1\n",
    "            if (time.time() - t_start) > train_time_limit: \n",
    "                print(f\"Stopping training after >{train_time_limit} seconds.\")\n",
    "                break\n",
    "        init_train_loss = init_train_sum / init_train_batches\n",
    "        t_train_done = time.time()\n",
    "\n",
    "        # avg loss on validation set\n",
    "        init_val_sum, init_val_batches = 0.0, 0\n",
    "        for vb in tqdm.tqdm(val_loader, desc=\"init‑val\"):\n",
    "            vb = move_to_device(vb, device)\n",
    "            vout = action_value_model(*extract_input(vb))\n",
    "            vo, vt = extract_output_target(\n",
    "                vb, vout, mean_val, std_val\n",
    "            )\n",
    "            init_val_sum += criterion(vo, vt).item()\n",
    "            init_val_batches += 1\n",
    "            if (time.time() - t_train_done) > val_time_limit:\n",
    "                print(f\"Stopping validation after >{val_time_limit} seconds.\")\n",
    "                break\n",
    "        init_val_loss = init_val_sum / init_val_batches\n",
    "        t_val_done = time.time()\n",
    "\n",
    "    # times\n",
    "    init_train_time = t_train_done - t_start\n",
    "    init_val_time   = t_val_done - t_train_done\n",
    "    init_total_time = t_val_done - t_start\n",
    "\n",
    "    # write initial row (epoch = None)\n",
    "    csv_writer.writerow([\n",
    "        -1,\n",
    "        init_train_loss,\n",
    "        init_val_loss,\n",
    "        init_train_time,\n",
    "        init_val_time,\n",
    "        init_total_time\n",
    "    ])\n",
    "    csv_file.flush()\n",
    "    print(\n",
    "        f\"Initial (-1) | \"\n",
    "        f\"Train {init_train_loss:.4f} | \"\n",
    "        f\"Val {init_val_loss:.4f} | \"\n",
    "        f\"Time {init_total_time:.1f}s (T {init_train_time:.1f}s | \"\n",
    "        f\"V {init_val_time:.1f}s)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88b91e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.get_last_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f88112",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, n_epochs):\n",
    "    t_start = time.time()\n",
    "\n",
    "    # ── TRAINING -----------------------------------------------------------\n",
    "    action_value_model.train()\n",
    "    sum_train_loss = torch.tensor(0.0, device=device)\n",
    "    n_train_batches = 0\n",
    "\n",
    "    t_batch_tqdm = tqdm.tqdm(\n",
    "        train_loader, \n",
    "        desc=f\"train {epoch}\" \n",
    "    )\n",
    "    for t_batch in t_batch_tqdm:\n",
    "        t_batch = move_to_device(t_batch, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model_out = action_value_model(*extract_input(t_batch))\n",
    "        outputs, targets = extract_output_target(t_batch, model_out, mean_val, std_val)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            raise ValueError(f\"Loss contains NaN or Inf values\")\n",
    "        \n",
    "        sum_train_loss += loss.detach()\n",
    "        n_train_batches += 1\n",
    "        t_batch_tqdm.set_postfix(av_loss=sum_train_loss.item()/n_train_batches)\n",
    "        if (time.time() - t_start) > train_time_limit: \n",
    "            print(f\"Stopping training after >{train_time_limit} seconds.\")\n",
    "            break\n",
    "        \n",
    "    t_batch_tqdm.close()\n",
    "\n",
    "    avg_train_loss = sum_train_loss.item() / n_train_batches\n",
    "    t_train_done = time.time()\n",
    "\n",
    "    # ── VALIDATION ---------------------------------------------------------\n",
    "    action_value_model.eval()\n",
    "    sum_val_loss = 0.0\n",
    "    n_val_batches = 0\n",
    "    with torch.no_grad():\n",
    "        v_batch_tqdm = tqdm.tqdm(\n",
    "            val_loader, \n",
    "            desc=f\"val {epoch}\" \n",
    "        )\n",
    "        for v_batch in v_batch_tqdm:\n",
    "            v_batch = move_to_device(v_batch, device)\n",
    "\n",
    "            v_out = action_value_model(*extract_input(v_batch))\n",
    "            v_outputs, v_targets = extract_output_target(v_batch, v_out, mean_val, std_val)\n",
    "            val_loss = criterion(v_outputs, v_targets)\n",
    "\n",
    "            if torch.isnan(val_loss) or torch.isinf(val_loss):\n",
    "                raise ValueError(f\"Loss contains NaN or Inf values\")\n",
    "            \n",
    "            sum_val_loss += val_loss\n",
    "            n_val_batches += 1\n",
    "            v_batch_tqdm.set_postfix(av_loss=sum_val_loss.item()/n_val_batches)\n",
    "            if (time.time() - t_train_done) > val_time_limit:\n",
    "                print(f\"Stopping validation after >{val_time_limit} seconds.\")\n",
    "                break\n",
    "\n",
    "        v_batch_tqdm.close()\n",
    "\n",
    "    avg_val_loss = sum_val_loss.item() / n_val_batches\n",
    "    t_val_done = time.time()\n",
    "\n",
    "    # Step the learning rate scheduler after each epoch\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # ── CSV logging ---------------------------------------------------------\n",
    "    train_time = t_train_done - t_start\n",
    "    val_time   = t_val_done   - t_train_done\n",
    "    total_time = t_val_done   - t_start\n",
    "\n",
    "\n",
    "    checkpoint_data = {\n",
    "        \"epoch\": epoch,\n",
    "        \"optim_state\": optimizer.state_dict(),\n",
    "        \"model_state\": action_value_model.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint_data, os.path.join(models_dir, f\"{model_name}_{epoch:06}.pt\"))\n",
    "    \n",
    "    csv_writer.writerow([\n",
    "        epoch,\n",
    "        avg_train_loss,\n",
    "        avg_val_loss,\n",
    "        train_time,\n",
    "        val_time,\n",
    "        total_time\n",
    "    ])\n",
    "    csv_file.flush()  # ensure data is written even if run aborts\n",
    "\n",
    "    # ── console printout ----------------------------------------------------\n",
    "    print(\n",
    "        f\"Epoch {epoch} | \"\n",
    "        f\"Train {avg_train_loss:.4f} | \"\n",
    "        f\"Val {avg_val_loss:.4f} | \"\n",
    "        f\"LR {scheduler.get_last_lr()[0]}\"\n",
    "    )\n",
    "# ── tidy‑up ----------------------------------------------------------------\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca4bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf5745",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47f5b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
